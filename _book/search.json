[{"path":"index.html","id":"the-golem-of-prague","chapter":"1 The Golem of Prague","heading":"1 The Golem of Prague","text":"Right now ’m just trying work seems 4 years reading statistical rethinking learning statistics. Bare bones right now hoping make thing moving forward. Going try keep context within natural resource framework (fisheries, hydrology, soils).won’t go Chapter 1 ’s mostly read . Great content outline rest book much put .","code":""},{"path":"small-worlds-and-large-worlds.html","id":"small-worlds-and-large-worlds","chapter":"2 Small Worlds and Large Worlds","heading":"2 Small Worlds and Large Worlds","text":"","code":""},{"path":"small-worlds-and-large-worlds.html","id":"small-worlds-and-large-worlds-1","chapter":"2 Small Worlds and Large Worlds","heading":"2.1 Small Worlds and Large Worlds","text":"great distinction small large world small world model large world world hope deploy . challenge statistical modeling elevated forgetting distinction.small world surprises important verify logic, making sure performs expected favorable assumptions.large world events expected imagined small world, e.g. coupling events, 0 understanding misleading, etc. essentially modeling adage ‘models wrong, useful.’ Just model small world makes logical sense doesn’t mean consistent large world. Richard says, ‘certainly warm comfort.’chapter ’ll start building Bayesian models. Bayesian models learn prior information super helpful small world. assumptions close reality, also great large world.","code":""},{"path":"small-worlds-and-large-worlds.html","id":"garden-of-forking-paths","chapter":"2 Small Worlds and Large Worlds","heading":"2.2 Garden of forking paths","text":"humble beginnings Bayesian inference: counting comparing possibilities. Richard compares Jorge Luis Borges’ short story “Garden Forking Paths.” short, life full paths exploring help make good inference. learn. prune. inference might give correct answer large world can guarantee best possible answer small world, given information fed .","code":""},{"path":"small-worlds-and-large-worlds.html","id":"counting-possibilities","chapter":"2 Small Worlds and Large Worlds","heading":"2.2.1 Counting possibilities","text":"’ll take play marble scenario use fish bucket. Let’s say stream either Bull Trout (salvelinus confluentus) Rainbow Trout (Oncorhynchus Mykiss) let’s say 4 samples stream. five different possibilities samples: mixed,\nFigure 2.1: Possibilities bucket.\nwant figure conjecture plausible, given evidence samples. Let’s say first 3 samples (replacement) get: Bull Trout, Rainbow Trout, Bull Trout (BT, RB, BT). plausible configuration bucket? Let’s consider one example; p2 (possibility #2) Figure 2.1 (BT, RB, RB, RB). forking paths comes handy can count ways possibility can happen (Figure 2.2), .e. possibility meaning 3 RB 1 BT bucket.\nFigure 2.2: Paths taken bucket forking fish.\nFigure 2.2 , can see 1 way getting bull trout first draw, 3 ways second 1 third. can now possibilities. Figure 2.2 shows 3 paths ways bag contain (BT, RB, RB, RB). Now great need test ways right? example, let’s say wanted see bucket RB!? Right, doesn’t make sense. pulled BT twice definitely isn’t possible. goes BT. Since paths don’t exist based reality, know way produce 0. Now know ways produce (BT, RB, RB, RB) 0’s, let’s look others Table 2.1 .Table 2.1: Conjectures Ways Produce Fish BucketWe can see based sample (RB, BT, BT, BT) 9 ways (RB, RB, BT, BT) 8 ways. 5 different conjectures give us different paths garden forking data tell us produce (BT, RB, BT). can see Table 2.1, just multiply intermediary steps instead counting ’ll see later really handy conjectures grow.","code":"## # A tibble: 4 x 5\n##      p1    p2    p3    p4    p5\n##   <dbl> <int> <int> <int> <dbl>\n## 1     0     1     1     1     1\n## 2     0     0     1     1     1\n## 3     0     0     0     1     1\n## 4     0     0     0     0     1"},{"path":"small-worlds-and-large-worlds.html","id":"combining-other-information","chapter":"2 Small Worlds and Large Worlds","heading":"2.2.2 Combining other information","text":"information relative plausibility conjecture? Many ways might important point can help us update conjectures. case ’s easy solution: just multiply counts.Let’s consider initial data call priors. Now let’s say draw another fish bucket ’s Bull Trout (BT). just start count paths, etc just multiply new observation ways produce given conjecture. turns methods mathematically identical (independent assumption). , ’ll just take prior counts based (BT, RB, BT) <=> (0, 3, 8, 9, 0) multiply new ways (0, 1, 2, 3, 4) (Table 2.2).Table 2.2: Updating priors new data.great new data arrives can just update!","code":""},{"path":"small-worlds-and-large-worlds.html","id":"from-counts-to-probability","chapter":"2 Small Worlds and Large Worlds","heading":"2.2.3 From counts to probability","text":"Counts great pedagogical example garden forking data works data grows counts become difficult work (imagine ten fish bucket). becomes easier compress formula use multiplication. Let’s look previous example now within formula speaking .\\[\n\\text{plausibility } \\ p \\ \\text{} \\ D_{new} \\ \\propto \\ \\text{ways} \\ p \\ \\text{can produce} \\ D_{new} \\ \\times \\ \\text{prior plausibility } p\n\\]can say now , “plausibility \\(p\\) (B, R, R, R) \\(D_{new}\\) (drawing B, R, B) proportional (\\(\\propto\\)) ways \\(p\\) (B, R, R, R) can produce \\(D_{new}\\) (drawing B, R, B) multiplied (\\(times\\)) prior plausibility \\(p\\) (B, R, R, R). really just formalizing previous section now can scale . Now go counts probabilities need divide rhs equation sum products (ways produce data), (Table 2.3).\\[\n\\text{plausibility } \\ p \\ \\text{} \\ D_{new} \\ = \\frac{\\text{ways} \\ p \\ \\text{can produce} \\ D_{new} \\ \\times \\ \\text{prior plausibility } p}{\\text{sum products}}\n\\]Table 2.3: counts probabilities… plausibilities?might thinking plausibility probability? Exactly. ’s confusing simply plausibilites probabilities come mathematical things probability theory. McElreath (2018) goes common names things probability theory:conjectured proportion bull trout (BT), \\(p\\), usually called parameter value.\n’s just way indexing possible explanations data.relative number ways value \\(p\\) can produce data usually called\nlikelihood. derived enumerating possible data sequences \nhappened eliminating sequences inconsistent \ndata.prior plausibility specific \\(p\\) usually called prior probability.new, updated plausibility specific \\(p\\) usually called \nposterior probability.– McElreath (2018)next section builds framework helps bring together simple toy example.","code":""},{"path":"small-worlds-and-large-worlds.html","id":"building-a-model","chapter":"2 Small Worlds and Large Worlds","heading":"2.3 Building a Model","text":"globe problem always reminded similar hydrology example predicting headwater streams (Erickson, Holden, Efta (2023)). can look streams across landscape similar look tossing globe. Instead trying understand proportion water globe, want know proportion streams within watershed.First need sample theoretical stream network across watershed just like globe get observation stream able generate posterior distribution. ’ll look National Hydrography Dataset Plus High Resolution (NHDPlus HR) raster Northwest Montana sample raster get idea proportion land water (Blodgett Johnson (2018)). ’s perfect ’ll see later starts converge pretty quickly give us clearer picture likelihood.\nFigure 2.3: Watershed NHDPlus HR streams\nRichard goes steps help us formulate Bayesian model workflow can done well ’s process (repeated). ’ll go step following sections.Data story: Motivate model narrating data might arise.Update: Educate model feeding data.Evaluate: statistical models require supervision, leading possibly model revision.– McElreath (2018)","code":"\nlibrary(terra)\n\nnhdplusHR <- rast('extdata/streamgridMT.tif')\nwatershed <- vect('extdata/watershed.shp') %>% project(crs(nhdplusHR))"},{"path":"small-worlds-and-large-worlds.html","id":"a-data-story","chapter":"2 Small Worlds and Large Worlds","heading":"2.3.1 A data story","text":"always great start lot times overlooked forgotten, .e. data generated. sometimes looking data way describe underlying property pattern looking gain knowledge underlying casual mechanisms drive underlying properites. Sometimes back forth can difficult (descriptive <-> causal) end model just trying give algorithm ’s built perform (remember golems). Thus, need understand story data generated important understanding model. addition, simulating data come handy helps get heart data generating process associated descriptive <-> causal linkages. simple summary might look stream proportion watershed model Data story.true proportion streams covering watershed \\(p\\).single random sample watershed probability \\(p\\) producing stream (S) observation.\nprobability 1 − \\(p\\) producing land (L) observation.random sample independent others.– McElreath (2018)much easier though see action sample update model understanding data story. ’ll go next section.","code":""},{"path":"small-worlds-and-large-worlds.html","id":"bayesian-updating","chapter":"2 Small Worlds and Large Worlds","heading":"2.3.2 Bayesian updating","text":"Remember ‘fish bucket’ example earlier followed paths come ways plausibilities, well ’s going process now ’ll just use random samples watershed instead. time sample watershed, ’ll update posterior plausibility taking prior multiplying likelihood. ’ll call Bayesian Updating can see process happening Figure 2.4. first, need get data (samples) show updates event., ’ll randomly sample stream raster Figure 2.3 get 9 samples either S (stream) L (land).vector sample9 get Land, Land, Land, Land, Land, Land, Land, Stream, Land.can see ’s pretty rare get stream land! first panel Figure 2.4 important shows prior (dashed line) uniform meaning give equal probability proportion stream watershed. give thought prior working example ’ll just stick uniform now. later chapters ’ll play prior see affects posterior distribution.\nFigure 2.4: Bayesian updating based random samples watershed. Notice get’s Land, knows can’t possible Land. Dashed line prior solid line likelihood.\n2.4 can see taking 50% probability getting ‘S’ ‘L’ (even though know ’s true…) updating based sample Land Stream. every time Land observed, peak plausibility curve moves left graph. repeats step changes likelihood depending priors.","code":"\nset.seed(2349)\n\nsample9 <- terra::spatSample(nhdplusHR, size = 9, method = \"random\", replace = TRUE)\n\n## just adding a S and L\nsample9$observations <- ifelse(is.na(sample9$streamgrid), 'Land', 'Stream')"},{"path":"small-worlds-and-large-worlds.html","id":"evaluate","chapter":"2 Small Worlds and Large Worlds","heading":"2.3.3 Evaluate","text":"Richard goes details small large world comparisons warns practitioner’s assumptions data model, e.g. assuming model correct model use data small world representation large world. also makes important point data drawn. example, proportion streams watershed doesn’t matter draw stream 2, 3, 8, 20, etc order doesn’t matter situations order can matter ’d want make sure model aware order! addition, make sure aware models purpose (prediction, inference, etc) check adequacy within domain.","code":""},{"path":"small-worlds-and-large-worlds.html","id":"components-of-the-model","chapter":"2 Small Worlds and Large Worlds","heading":"2.4 Components of the model","text":"Now fun part, open machine! ’ll learn machine works, part part. help us connect model (golem) statistical properties, e.g. probability theory (distributions).","code":""},{"path":"small-worlds-and-large-worlds.html","id":"variables","chapter":"2 Small Worlds and Large Worlds","heading":"2.4.1 Variables","text":"Richard explains variables symbols take different values things wish infer (proportions rates) addition things actually observe like data. stream proportion example three variables; \\(p\\), \\(S\\), \\(L\\).first variable \\(p\\) proportion streams watershed. variable observed instead inferred data. Richard explains usually called parameter. variables just observed variables samples, e.g. stream land.","code":""},{"path":"small-worlds-and-large-worlds.html","id":"definitions","chapter":"2 Small Worlds and Large Worlds","heading":"2.4.2 Definitions","text":"variables need define . help us achieve ways can count paths considering variables used (observed unobserved). think define context little abstract Richard says really means walking paths. ’ll define observe unobserved variables help us abstract.","code":""},{"path":"small-worlds-and-large-worlds.html","id":"observed-variables","chapter":"2 Small Worlds and Large Worlds","heading":"2.4.2.1 Observed variables","text":"simply trying figure best way represent data generating process \\(S\\) \\(L\\) within watershed finding likely \\(p\\) given data. Typically data follows common probability distribution function (continuous pdf) probability mass function (discrete pmf) can use count us.example, two possible events: streams (\\(S\\)) land (\\(L\\)) also took 9 samples (\\(N\\)) watershed. like know plausible get sample configuration given amount samples.Couple things first, need make sure samples identically distributed independent (iid). ensures don’t bring sampling bias might impact machine (golem). Lucky us already distribution ! ’s call binomial distribution.\\[\nPr(W,L|p)=\\frac{(W+L)!}{W!L!}p^{W}(1-p)^{L}\n\\]side-bar–might thinking another distribution sounds similar also deals binary outcomes called Bernoulli. Bernoulli distribution probability \\(p\\) observing binary event.\\[\nPr(k|p)=p^{k}(1-p)^{1-k} \\\\ Pr(W,L|p)=p^{W}(1-p)^{L}\n\\]\nsuper similar binomial Bernoulli special case binomial essentially binomial one event!’s nice equation ’s already function R!relative number ways get 1 stream, holding \\(p\\) 0.5 \\(N = W+L\\) 9. can now summarize model. can change prob argument dbinom() see changes results. Richard talks Maximum entropy binomial distribution represents counting binary events explain Chapter 10.","code":"\ndbinom(1, size = 9, prob = .5)## [1] 0.01757813"},{"path":"small-worlds-and-large-worlds.html","id":"unobserved-variables","chapter":"2 Small Worlds and Large Worlds","heading":"2.4.2.2 Unobserved variables","text":"Since \\(p\\) observed ’ll call parameter even though can’t observe still need define somehow. can think parameters questions want answer: difference means groups, strong two variables associated, much variation measurements, etc. parameters, ’ll need assign prior. Just like previous section gave first prior uniform distribution (equally likely) ’ll need build models. might asking, “?”can think priors ways help model learn (engineering assumptions) imparting knowledge reflect phenomenon (scientific assumtions). Essentially, priors can subjective non-subjective follow similar workflows, e.g. analyze data using prior information. Priors permanent law. assumptions treated like , meaning challenged explored!McElreath (2018) goes nice distinction parameter data also provides great summary shouldn’t get wrapped within Bayesian analyis.Rethinking: Datum parameter? typical conceive data parameters completely different kinds entities. Data measured known; parameters unknown must estimated data. Usefully, Bayesian framework distinction datum parameter fundamental. Sometimes observe variable, sometimes . case, distribution function applies, even though didn’t observe variable. result, assumption can look like “likelihood” “prior,” depending upon context, without change model. Much later book (Chapter 15), ’ll see exploit deep identity certainty (data) uncertainty (parameters) incorporate measurement error missing\ndata modeling.– McElreath (2018)","code":""},{"path":"small-worlds-and-large-worlds.html","id":"a-model-is-born","chapter":"2 Small Worlds and Large Worlds","heading":"2.4.3 A model is born","text":"Now ’ve done work define everything, let’s put together! just notation describing ’ve already done. observed variable \\(W\\) described binomial distribution size \\(N\\) probability \\(p\\).\\[\nW \\sim \\text{Binomial}(N,p)\n\\]unobserved parameter \\(p\\) just saying uniform (flat) ’s entire range (zero one):\\[\np \\sim \\text{Uniform}(0,1)\n\\]\nbest prior since know area way land stream (Figure 2.3. Later ’ll start play priors instead leaving flat.","code":""},{"path":"small-worlds-and-large-worlds.html","id":"making-the-model-go","chapter":"2 Small Worlds and Large Worlds","heading":"2.5 Making the model go","text":"’ll start getting guts Bayes’ theorem derive posterior don’t get discouraged ! really just process now derivation notation added gel rules probability theory. posterior distribution ’re going derive contains relative plausibility different parameter values, conditional data. First ’ll look joint probability \\(S, L, p\\).saying probability \\(S, L, p\\) product probability \\(Pr(S,L|p)\\) likelihood times prior probability \\(p\\). like saying probability rain cold day equal probability rain, ’s cold, times probability ’s cold (McElreath (2018)).’s also true reverse logic, e.g. reverse \\(p\\) \\(rhs\\).Meaning probability rain cold day equal probability cold, ’s raining, times probability rain (McElreath (2018)).Now equations equal, can just set equal .Now little algebra,Voila! now wanted first place, e.g. \\(Pr(p|S,L)\\). Bayes’ theorem, saying probability \\(p\\) given data (\\(S,L\\)), equal likelihood (plausibility data conditional \\(p\\)) times prior plausibility, divided average probability data.\\[\n\\text{Posterior}=\\frac{\\text{Probability data} \\times \\text{Prior}}{\\text{Average probability data}}\n\\]McElreath (2018), talks denominator Bayes’ theorem particularly confusing can different names (evidence, average likelihood) literally average probability data. ’s meant standardize posterior making sure \\(rhs\\) sums one.\\[\nPr(W,L) = E\\bigl(Pr(W,L|p)\\big) = \\int{Pr(W,L|p)Pr(p)dp}\n\\]key take away equation integral helps standardize average probability values sum one. Remember forking path’s example ways produce \\(\\times\\) prior plausibility, divided sum products didn’t probability. thing goes estimate, continuous discrete, start likelihood \\(\\times\\) prior remember likelihood priors just plausibilities means doesn’t necessarily need zero 1 (absolutely can likely )!McElreath (2018) finishes section showing Figure 2.5 can keep likelihood change prior effect posterior varies.\nFigure 2.5: Graph showing posterior varying choice prior.\n","code":""},{"path":"small-worlds-and-large-worlds.html","id":"motors","chapter":"2 Small Worlds and Large Worlds","heading":"2.5.1 Motors","text":"section, McElreath (2018) dives machine(s) makes possible compute posterior distribution, namely:Grid approximationQuadratic approximationMarkov chain Monte Carlo (MCMC)reason need little help start adding complexity models mathematics becomes difficult possible! addition, models like linear regression certain formalities associated constrain choice prior might help math useful bayesian approach. methods help us approximate posterior distribution necessary tool tool-kit bayesian analysis.","code":""},{"path":"small-worlds-and-large-worlds.html","id":"grid-approximation","chapter":"2 Small Worlds and Large Worlds","heading":"2.5.2 Grid approximation","text":"Deemed one simplest sampling techniques grid approximation. goal take continuous grid (parameters) sample finite grid values. means particular value \\(p'\\) going multiply likelihood prior point. machine repeat starts get clearer picture posterior distribution.method comes cost though really inefficient start scaling problem . now, great stepping stone see machine action.McElreath (2018) gives recipe grid approximation using stream watershed example.Define grid. means decide many points use estimating \nposterior, make list parameter values grid.Compute value prior parameter value grid.Compute likelihood parameter value.Compute unstandardized posterior parameter value, multiplying \nprior likelihood.Finally, standardize posterior, dividing value sum values.– McElreath (2018)code steps :\nFigure 2.6: Posterior distributions using samples 5 20.\nsamples, precision ’ll get. Let’s look one 1,000 samples.\nFigure 2.7: Posterior distribution using 1,000 samples.\nprior just 1, dbinom right? Yes. play around prior show effects posterior.\nFigure 2.8: Showing varying effect prior posterior\n","code":"\nd <- tibble(p_grid = seq(from = 0, to = 1, length.out = 20),      # define grid\n           prior  = 1)  %>%                                       # define prior\n     mutate(likelihood = dbinom(1, size = 9, prob = p_grid)) %>%  # compute likelihood at each value in grid\n     mutate(unstd_posterior = likelihood * prior) %>%             # compute product of likelihood and prior\n     mutate(posterior = unstd_posterior / sum(unstd_posterior))   # standardize the posterior, so it sums to 1\nprior  = ifelse(p_grid > 0.25, 1, 0.05)\n# or\nprior  = exp( -5*abs( p_grid - 0.5 ))"},{"path":"small-worlds-and-large-worlds.html","id":"quad","chapter":"2 Small Worlds and Large Worlds","heading":"2.5.3 Quadratic approximation","text":"Due downfalls grid approximation (scalability) methods ’ll want learn well. One useful approach quadratic approximation. look 2.7, can see little imagination posterior resembles Gaussian (normal) shape. ’ll take advantage shape posterior apply Gaussian approximation aka quadratic. logarithm Gaussain distribution forms parabola parabola quadratic function. can work really well linear regression efficient ’ll use first following sections get complex models. However, ’ll see real quickly approximation might break reading first step.McElreath (2018) lays couple steps working quadratic approximation ’ll dive .Find posterior mode. usually accomplished optimization algorithm,\nprocedure virtually “climbs” posterior distribution, \nmountain. golem doesn’t know peak , know slope\nfeet. many well-developed optimization procedures, \nclever simple hill climbing. try find peaks.find peak posterior, must estimate curvature near \npeak. curvature sufficient compute quadratic approximation \nentire posterior distribution. cases, calculations can done analytically,\nusually computer uses numerical technique instead.– McElreath (2018)Within rethinking package, McElreath (2018) function quap us. just need provide necessary information (data params) function rest.output see summary stats approximation. can already see data bounds extend beyond theoretically possible (89% percentile interval less 0 lower bound). time huge problem something need think , .e. assumptions data want exactly estimand. summary results saying: assuming posterior Gaussian, maximized 0.11, standard deviation 0.10. ’ll explore later methods now ’ll just go Gaussian.\nFigure 2.9: Graph showing analytical approach (beta distribution) approximated approach (quadratic approximation).\n2.9 , can see decent job finding shape since assumes Gaussian ’s tails thin well beyond bounds parameter (gives positive probability 0…)!egregious approximation get’s better better samples. Figure 2.10 shows quadratic gets better better compare grid approximation. good news also reason small sample size frequentist statistics can troublesome another method (MCMC) can help, introduced next.\nFigure 2.10: samples increase, approximation starts reflect true distribution.\nAnother great Rethinking section McElreath (2018), making think comparison non-Bayesian parameter estimation.Rethinking: Maximum likelihood estimation. quadratic approximation, either uniform\nprior lot data, often equivalent maximum likelihood estimate (MLE) \nstandard error. MLE common non-Bayesian parameter estimate. correspondence Bayesian approximation common non-Bayesian estimator blessing\ncurse. blessing, allows us re-interpret wide range published non-Bayesian\nmodel fits Bayesian terms. curse, maximum likelihood estimates curious\ndrawbacks, quadratic approximation can share . ’ll explore later chapters.– McElreath (2018)","code":"\nlibrary(rethinking)\n\nstream_qa <- quap(\nalist(\nS ~ dbinom( S+L ,p) , # binomial likelihood\np ~ dunif(0,1) # uniform prior\n) ,\ndata=list(S=1,L=8) )\n# display summary of quadratic approximation\n\nprecis(stream_qa)##        mean        sd        5.5%     94.5%\n## p 0.1111137 0.1047512 -0.05629886 0.2785263"},{"path":"small-worlds-and-large-worlds.html","id":"markov-chain-monte-carlo","chapter":"2 Small Worlds and Large Worlds","heading":"2.5.4 Markov chain Monte Carlo","text":"issues talked (time/efficiency, normality assumptions) rear head models get complex (mixed-effects models, maximizing posterior). markov chain monte carlo (MCMC) comes handy. MCMC said big part resurgence Bayesian data analysis began 1990’s.Instead trying approximate posterior directly, MCMC samples posterior. always reminding bootstrapping James et al. (2013) ’ll dive Chapter 9 see different .example wet appetite MCMC.\nFigure 2.11: Comparing analytical approach (Beta) sampling (MCMC).\n","code":"\nn_samples <- 1000\np <- rep( NA , n_samples )\np[1] <- 0.5\nS <- 1\nL <- 8\n\nfor ( i in 2:n_samples ) {\np_new <- rnorm( 1 , p[i-1] , 0.1 )\nif ( p_new < 0 ) p_new <- abs( p_new )\nif ( p_new > 1 ) p_new <- 2 - p_new\nq0 <- dbinom( S , S+L , p[i-1] )\nq1 <- dbinom( S , S+L , p_new )\np[i] <- ifelse( runif(1) < q1/q0 , p_new , p[i-1] )\n}"},{"path":"small-worlds-and-large-worlds.html","id":"practice","chapter":"2 Small Worlds and Large Worlds","heading":"2.6 Practice","text":"2E1. expressions correspond statement: probability rain Monday?Pr(rain)Pr(rain|Monday)Pr(Monday|rain)Pr(rain, Monday)/ Pr(Monday)Statement (2) (4) correct. (4) definition conditional probability, \\[\\text{Pr(rain|Monday)}=\\frac{\\text{Pr(rain, Monday)}}{\\text{Pr(Monday)}} \\\\ \\text{Conditional Probability, } P(|B) = \\frac{\\cap B}{P(B)}\\]2E2. following statements corresponds expression: Pr(Monday|rain)?probability rain Monday.probability rain, given Monday.probability Monday, given raining.probability Monday raining.Statement (3) correct answer. (4) seems likely ’s meaning probability theory.probability Monday, given raining:\ndenoted \\(\\text{Pr(Monday∣Raining}\\)), represents probability Monday given information currently raining. example conditional probability.probability Monday, given raining:\ndenoted \\(\\text{Pr(Monday∣Raining}\\)), represents probability Monday given information currently raining. example conditional probability.probability Monday raining:\ndenoted \\(\\text{Pr(Monday}\\cap \\text{Raining)}\\), represents probability events, Monday raining, occur simultaneously. example intersection (commonly called joint probability) two events.probability Monday raining:\ndenoted \\(\\text{Pr(Monday}\\cap \\text{Raining)}\\), represents probability events, Monday raining, occur simultaneously. example intersection (commonly called joint probability) two events.2E3. expressions correspond statement: probability Monday, given raining?Pr(Monday|rain)Pr(rain|Monday)Pr(rain|Monday) Pr(Monday)Pr(rain|Monday) Pr(Monday)/ Pr(rain)Pr(Monday|rain) Pr(rain)/ Pr(Monday)(4) correct. (4) correct Bayes’ theorem: \\[\\frac{\\text{Pr(rain|Monday) Pr(Monday)}}{\\text{Pr(rain)}}=\\text{Pr(Monday|rain)}\\]2E4. Bayesian statistician Bruno de Finetti (1906–1985) began book probability theory\ndeclaration: “PROBABILITY EXIST.” capitals appeared original, \nimagine de Finetti wanted us shout statement. meant probability device\ndescribing uncertainty perspective observer limited knowledge; \nobjective reality. Discuss stream occurrence example chapter, light statement. \nmean say “probability stream 0.11?”take mean parameter (unobserved) estimated data (observed) probability 0.11, belief uncertainty small world. ’s important distinction McElreath (2018) talks unobserved variables probability reality instead parameter use data (information) estimate . contrary frequentist statistics probabilities frequencies observed objective properties physical world.Medium.2M1. Recall globe tossing model chapter. Compute plot grid approximate\nposterior distribution following sets observations. case, assume uniform\nprior p. L, L, LL, L, L, SS, L, L, S, L, L, L2M2. Now assume prior p equal zero p > 0.5 positive constant \np <= 0.5. compute plot grid approximate posterior distribution sets \nobservations problem just .2M3. Suppose two watersheds, one humid region one arid region. humid watershed 20% covered\nstreams. arid watershed 100% land. suppose one watersheds—don’t know\n—sampled produced “Land” observation. Assume watershed equally\nlikely sampled. Show posterior probability watershed humid, conditional \nseeing “land” (Pr(humid|land)), 0.44.Bayes’ theorem: \\[\\text{Pr(humid|land)} = \\frac{\\text{Pr(land|humid) Pr(humid)}}{\\text{Pr(land)}} \\\\ \\text{Pr(humid|land)} = \\frac{0.80\\times 0.50}{(0.50\\times 0.80) + (1-0.50)*1}\\]2M4. Suppose deck three cards. card two sides, side either\nblack white. One card two black sides. second card one black one white side. \nthird card two white sides. Now suppose three cards placed bag shuffled. Someone\nreaches bag pulls card places flat table. black side shown facing ,\ndon’t know color side facing . Show probability side \nalso black 2/3. Use counting method (Section 2 chapter) approach problem. \nmeans counting ways card produce observed data (black side facing \ntable).Table 2.4: Updating priors new data.2M5. Now suppose four cards: B/B, B/W, W/W, another B/B. suppose card \ndrawn bag black side appears face . calculate probability \nside black.Table 2.5: Updating priors new data.2M6. Imagine black ink heavy, cards black sides heavier cards white\nsides. result, ’s less likely card black sides pulled bag. assume\nthree cards: B/B, B/W, W/W. experimenting number times, conclude \nevery way pull B/B card bag, 2 ways pull B/W card 3 ways \npull W/W card. suppose card pulled black side appears face . Show \nprobability side black now 0.5. Use counting method, .Table 2.6: Updating priors new data.2M7. Assume original card problem, single card showing black side face . \nlooking side, draw another card bag lay face table. face\nshown new card white. Show probability first card, one showing\nblack side, black side now 0.75. Use counting method, can. Hint: Treat\nlike sequence globe tosses, counting ways see observation, possible\nfirst card.Table 2.7: Updating priors new data.Hard2H1. Suppose two species panda bear. equally common wild live\nplaces. look exactly alike eat food, yet genetic assay\ncapable telling apart. differ however family sizes. Species gives birth twins\n10% time, otherwise birthing single infant. Species B births twins 20% time, otherwise\nbirthing singleton infants. Assume numbers known certainty, many years field\nresearch.\nNow suppose managing captive panda breeding program. new female panda\nunknown species, just given birth twins. probability next birth\nalso twins?2H2. Recall facts problem . Now compute probability panda \nspecies , assuming observed first birth twins.2H3. Continuing previous problem, suppose panda mother second birth\ntwins, singleton infant. Compute posterior probability panda \nspecies .2H4. common boast Bayesian statisticians Bayesian inference makes easy use \ndata, even data different types.\nsuppose now veterinarian comes along new genetic test claims can\nidentify species mother panda. test, like tests, imperfect. information test:• probability correctly identifies species panda 0.8.• probability correctly identifies species B panda 0.65.vet administers test panda tells test positive species . First\nignore previous information births compute posterior probability \npanda species . redo calculation, now using birth data well.","code":"\n# probability of land, given humid region\np_lh <- 0.8\n\n# probability of land, given arid region\np_la <- 1.0\n\n# probability of humid region\np_h <- 0.5\n\n# probability of land\np_l <- (p_h * p_lh) + ((1 - p_h) * p_la)\n\n# probability of humid region, given land (using Bayes' Theorem)\np_hl <- (p_lh * p_h) / p_l\np_hl## [1] 0.4444444\nn_b <- function(x) {\n  rowSums(x == \"B\")\n}\n\nt <-\n  # for the first four columns, `p_` indexes position\n  tibble(p_1 = rep(c(\"B\", \"W\"), times = c(2,1)),\n         p_2 = rep(c(\"B\", \"W\"), times = c(1, 2))) %>% \n  mutate(`ways to produce` = n_b(.),\n         Prior = 1,\n         Probability = round(Prior*`ways to produce`/sum(`ways to produce`), 2))\n\nt %>% \n  knitr::kable(caption = 'Updating priors with new data.') \nt <-\n  # for the first four columns, `p_` indexes position\n  tibble(p_1 = rep(c(\"B\", \"W\"), times = c(3,1)),\n         p_2 = rep(c(\"B\", \"W\"), times = c(2, 2))) %>% \n  mutate(`ways to produce` = n_b(.),\n         Prior = 1,\n         Probability = round(Prior*`ways to produce`/sum(`ways to produce`), 2))\n\nt %>% \n  knitr::kable(caption = 'Updating priors with new data.') \n# add the two black cards together to get 80%\nt <-\n  # for the first four columns, `p_` indexes position\n  tibble(p_1 = rep(c(\"B\", \"W\"), times = c(2,1)),\n         p_2 = rep(c(\"B\", \"W\"), times = c(1, 2))) %>% \n  mutate( `ways to produce` = n_b(.),\n          `new ways to produce` = c(1,2,3)*`ways to produce`,\n         Probability = round((`new ways to produce`)/sum(`new ways to produce`),2))\n\nt %>% \n  knitr::kable(caption = 'Updating priors with new data.') \nt <-\n  # for the first four columns, `p_` indexes position\n  tibble(p_1 = rep(c(\"B\", \"W\"), times = c(2,1)),\n         p_2 = rep(c(\"B\", \"W\"), times = c(1, 2))) %>% \n  mutate( `ways to produce 1st` = n_b(.),\n          `ways to produce 2nd` = c(3,2,3),\n         Probability = round((`ways to produce 1st`*`ways to produce 2nd`)/sum(`ways to produce 2nd`),2)\n         )\n\nt %>% \n  knitr::kable(caption = 'Updating priors with new data.') \np_a = 0.1\np_b = 0.2\n\nlikelihood <- c(p_a, p_b)\nprior <- c(1,1)\nposterior <- likelihood*prior\nposterior <- posterior/sum(posterior)\nposterior## [1] 0.3333333 0.6666667\n(posterior[1]*p_a) + (posterior[2]*p_b)## [1] 0.1666667\n# probability of species A\np_a <- 0.5\n\n# probability of twins, given species A\np_ta <- 0.1\n\n# probability of twins, given species B\np_tb <- 0.2\n\n# probability of twins\np_t <- (p_a * p_ta) + ((1 - p_a) * p_tb)\n\n# probability of species A, given twins (using Bayes' Theorem)\n# (note this is equivalent to `posterior[1]` above)\np_at <- (p_ta * p_a) / p_t\np_at## [1] 0.3333333\n# probability of species A\np_a <- 0.5\n\n# probability of singleton, given species A with prior of having twin\np_sa <- 0.9*0.1\n\n# probability of singleton, given species B with prior of having twin\np_sb <- 0.8*0.2\n\n# probability of singletons\np_s <- (p_a * p_sa) + ((1 - p_a) * p_sb)\n\n# probability of species A, given singleton (using Bayes' Theorem)\n# (note this is equivalent to `posterior[1]` above)\np_as <- (p_sa * p_a) / p_s\np_as## [1] 0.36\n#Pr(a|test) = Pr(a)*Pr(test|a)\n\np_ta <- 0.8\n\np_tb <- 0.65\n\np_a <- 0.5\n\ntop <- 0.8*0.5\n\nbottom <- (0.8*0.5) + (0.35*0.5)\n\np_ap <- top/bottom\n\n# now add birth data\n\na_likelihood <- 0.1 * (1 - 0.1)\nb_likelihood <- 0.2 * (1 - 0.2)\n\n# compute posterior probabilities, using test result as prior\nlikelihood <- c(a_likelihood, b_likelihood)\nprior <- c(p_ap, (1 - p_ap))\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\nposterior[1]## [1] 0.5625"},{"path":"sampling-the-imaginary.html","id":"sampling-the-imaginary","chapter":"3 Sampling the Imaginary","heading":"3 Sampling the Imaginary","text":"McElreath (2018) starts chapter classic example Bayes’ theorem can confusing people come right probability given result model test example. uses example vampires ’ll stick soil, water fish theme build stream proportion model. Erickson, Holden, Efta (2023), ’ll use results Topoclimatic model., models accuracy get 83.5% \\(\\text{Pr(positive model result|stream)} = 0.835\\). ’s decent test good job predicting headwater streams makes mistakes, ’ll call false positives. false positive rate model \\(\\text{Pr(positive test result|land)} = 0.16\\). final bit information proportion streams across landscape \\(\\text{Pr(stream)} = 0.11\\), generated Chapter 2.5.3 quadratic approximation.Now can produce Bayes’ theorem,\\[\n\\text{Pr(stream|positive)} = \\frac{\\text{Pr(positive|stream)}\\times\\text{Pr(stream)}}{\\text{Pr(positive)}}\n\\]results 39% chance positive result stream. seems really counterintuitive! can model 83.5% accuracy returns positive result 39% chance result correct. response ’re trying predict rare, makes difficult get true cases due false positives. Play around numbers ’ll see rareness condition makes unlikely actually positive result.\nFigure 3.1: Using Bayes’ calculate probability stream positive (y-axis) given sequence proportion streams (x-axis).\nway present problem make intuitive, however.\nSuppose instead reporting probabilities, , tell following:watershed 100,000 acres, 11,000 streams.watershed 100,000 acres, 11,000 streams.11,000 streams, 9,185 test positive stream.11,000 streams, 9,185 test positive stream.89,000 acres, 14,240 test positive stream.89,000 acres, 14,240 test positive stream.Now tell , test 100,000 acres, proportion test positive \nstream actually streams? Many people, although certainly people, find \npresentation lot easier. Now can just count number streams test positive:\n14240 + 9185 = 23425. 23425 positive tests, 9185 real streams, \nimplies:\\[\n\\text{Pr(stream|positive)} = \\frac{9185}{23425} \\approx 0.087\n\\]’s exactly answer , without seemingly arbitrary rule.– McElreath (2018)McElreath (2018) comments misleading teaching moment Bayes’, since merely frequencies statistician arrive Bayes’ theorem explain . However, thinking human psychology might actually interpret frequencies counts better real world don’t ever see probability everyone can see counts. Richard prefaces coming chapter can use realizations sampling posterior counting. helps tremendously posteriors might easy integral calculus . addition, multiple parameters issue becomes troublesome even unattainable traditional approaches.","code":"\nPr_positive_stream <- 0.835\nPr_positive_land <- 0.16\nPr_stream <- 0.11\nPr_positive <- Pr_positive_stream*Pr_stream + Pr_positive_land*(1-Pr_stream)\n\n(Pr_stream_positive <- (Pr_positive_stream*Pr_stream)/Pr_positive)## [1] 0.3921025"},{"path":"sampling-the-imaginary.html","id":"sampling-from-a-grid-approximate-posterior","chapter":"3 Sampling the Imaginary","heading":"3.1 Sampling from a grid-approximate posterior","text":"start going back sampling watershed stream example.Now draw posterior, say 10,000 samples. sample grab proportion associated posterior probability. one sample likely higher proportion .","code":"\np_grid <- seq( from=0 , to=1 , length.out=1000 )\nprob_p <- rep( 1 , 1000 )\nprob_data <- dbinom( 1 , size=9 , prob=p_grid )\nposterior <- prob_data * prob_p\nposterior <- posterior / sum(posterior)\nsamples <- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE)\np1 <- plot(samples)\np2 <- dens(samples)"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
