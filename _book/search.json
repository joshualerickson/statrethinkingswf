[{"path":"index.html","id":"the-golem-of-prague","chapter":"1 The Golem of Prague","heading":"1 The Golem of Prague","text":"Right now ’m just trying work seems 4 years reading statistical rethinking learning statistics. Bare bones right now hoping make thing moving forward. Going try keep context within natural resource framework (fisheries, hydrology, soils).won’t go Chapter 1 ’s mostly read . Great content outline rest book much put .","code":""},{"path":"small-worlds-and-large-worlds.html","id":"small-worlds-and-large-worlds","chapter":"2 Small Worlds and Large Worlds","heading":"2 Small Worlds and Large Worlds","text":"","code":""},{"path":"small-worlds-and-large-worlds.html","id":"small-worlds-and-large-worlds-1","chapter":"2 Small Worlds and Large Worlds","heading":"2.1 Small Worlds and Large Worlds","text":"great distinction small large world small world model large world world hope deploy . challenge statistical modeling elevated forgetting distinction.small world surprises important verify logic, making sure performs expected favorable assumptions.large world events expected imagined small world, e.g. coupling events, priori understanding misleading, etc. essentially modeling adage ‘models wrong, useful’. Just model small world makes logical sense doesn’t mean consistent large world. Richard says, ‘certainly warm comfort.’chapter ’ll start building Bayesian models. Bayesian models learn prior information super helpful small world. assumptions close reality, also great large world.","code":""},{"path":"small-worlds-and-large-worlds.html","id":"garden-of-forking-paths","chapter":"2 Small Worlds and Large Worlds","heading":"2.2 Garden of forking paths","text":"humble beginnings Bayesian inference: counting comparing possibilities. Richard compares Jorge Luis Borges’ short story “Garden Forking Paths.” short, life full paths exploring help make good inference. learn. prune. inference might give correct answer large world can guarantee best possible answer small world, given information fed .","code":""},{"path":"small-worlds-and-large-worlds.html","id":"counting-possibilities","chapter":"2 Small Worlds and Large Worlds","heading":"2.2.1 Counting possibilities","text":"’ll take play marble scenario use fish bucket. Let’s say stream either Bull Trout (salvelinus confluentus) Rainbow Trout (Oncorhynchus Mykiss) let’s say 4 samples stream. five different possibilities samples: mixed,\nFigure 2.1: Possibilities bucket.\nwant figure conjecture plausible, given evidence samples. Let’s say first 3 samples (replacement) get: Bull Trout, Rainbow Trout, Bull Trout (BT, RB, BT). plausible configuration bucket? Let’s consider one example; p2 (possibility #2) Figure 2.1 (BT, RB, RB, RB). forking paths comes handy can count ways possibility can happen (Figure 2.2), .e. possibility meaning 3 RB 1 BT bucket.\nFigure 2.2: Paths taken bucket forking fish.\nFigure 2.2 , can see 1 way getting bull trout first draw, 3 ways second 1 third. can now possibilities. Figure 2.2 shows 3 paths ways bag contain (BT, RB, RB, RB). Now great need test ways right? example, let’s say wanted see bucket RB!? Right, doesn’t make sense. pulled BT twice definitely isn’t possible. goes BT. Since paths don’t exist based reality, know way produce 0. Now know ways produce (BT, RB, RB, RB) 0’s, let’s look others Table 2.1 .Table 2.1: Conjectures Ways Produce Fish BucketWe can see based sample (RB, BT, BT, BT) 9 ways (RB, RB, BT, BT) 8 ways. 5 different conjectures give us different paths garden forking data tell us produce (BT, RB, BT). can see Table 2.1, just multiply intermediary steps instead counting ’ll see later really handy conjectures grow.","code":"## # A tibble: 4 × 5\n##      p1    p2    p3    p4    p5\n##   <dbl> <int> <int> <int> <dbl>\n## 1     0     1     1     1     1\n## 2     0     0     1     1     1\n## 3     0     0     0     1     1\n## 4     0     0     0     0     1"},{"path":"small-worlds-and-large-worlds.html","id":"combining-other-information","chapter":"2 Small Worlds and Large Worlds","heading":"2.2.2 Combining other information","text":"information relative plausibility conjecture? Many ways might important point can help us update conjectures. case ’s easy solution: just multiply counts.Let’s consider initial data call priors. Now let’s say draw another fish bucket ’s Bull Trout (BT). just start count paths, etc just multiply new observation ways produce given conjecture. turns methods mathematically identical (independent assumption). , ’ll just take prior counts based (BT, RB, BT) <=> (0, 3, 8, 9, 0) multiply new ways (0, 1, 2, 3, 4) (Table 2.2).Table 2.2: Updating priors new data.great new data arrives can just update!","code":""},{"path":"small-worlds-and-large-worlds.html","id":"from-counts-to-probability","chapter":"2 Small Worlds and Large Worlds","heading":"2.2.3 From counts to probability","text":"Counts great pedagogical example garden forking data works data grows counts become difficult work (imagine ten fish bucket). becomes easier compress formula use multiplication. Let’s look previous example now within formula speaking .\\[\n\\text{plausibility } \\ p \\ \\text{} \\ D_{new} \\ \\propto \\ \\text{ways} \\ p \\ \\text{can produce} \\ D_{new} \\ \\times \\ \\text{prior plausibility } p\n\\]can say now , “plausibility \\(p\\) (B, R, R, R) \\(D_{new}\\) (drawing B, R, B) proportional (\\(\\propto\\)) ways \\(p\\) (B, R, R, R) can produce \\(D_{new}\\) (drawing B, R, B) multiplied (\\(times\\)) prior plausibility \\(p\\) (B, R, R, R). really just formalizing previous section now can scale . Now go counts probabilities need divide rhs equation sum products (ways produce data), (Table 2.3).\\[\n\\text{plausibility } \\ p \\ \\text{} \\ D_{new} \\ = \\frac{\\text{ways} \\ p \\ \\text{can produce} \\ D_{new} \\ \\times \\ \\text{prior plausibility } p}{\\text{sum products}}\n\\]Table 2.3: counts probabilities… plausibilities?might thinking plausibility probability? Exactly. ’s confusing simply plausibilites probabilities come mathematical things probability theory. McElreath (2018) goes common names things probability theory:conjectured proportion bull trout (BT), \\(p\\), usually called parameter value.\n’s just way indexing possible explanations data.relative number ways value \\(p\\) can produce data usually called\nlikelihood. derived enumerating possible data sequences \nhappened eliminating sequences inconsistent \ndata.prior plausibility specific \\(p\\) usually called prior probability.new, updated plausibility specific \\(p\\) usually called \nposterior probability.– McElreath (2018)next section builds framework helps bring together simple toy example.","code":""},{"path":"small-worlds-and-large-worlds.html","id":"building-a-model","chapter":"2 Small Worlds and Large Worlds","heading":"2.3 Building a Model","text":"globe problem always reminded similar hydrology example predicting headwater streams (Erickson, Holden, Efta (2023)). can look streams across landscape similar look tossing globe. Instead trying understand proportion water globe, want know proportion streams within watershed.First need sample theoretical stream network across watershed just like globe get observation stream able generate posterior distribution. ’ll look National Hydrography Dataset Plus High Resolution (NHDPlus HR) raster Northwest Montana sample raster get idea proportion land water (Blodgett Johnson (2018)). ’s perfect ’ll see later starts converge pretty quickly give us clearer picture likelihood.\nFigure 2.3: Watershed NHDPlus HR streams\nRichard goes steps help us formulate Bayesian model workflow can done well ’s process (repeated). ’ll go step following sections.Data story: Motivate model narrating data might arise.Update: Educate model feeding data.Evaluate: statistical models require supervision, leading possibly model revision.– McElreath (2018)","code":"\nlibrary(terra)\n\nnhd <- rast('Z:/GIT/stat_rethinking/stat_rethinking_2023/nhdStrOrdRast.tif')\nsutton <- vect('Z:/GIT/stat_rethinking/stat_rethinking_2023/sutton.shp') %>% project(crs(nhd))\nnhd_sutton <- crop(nhd, sutton, mask = T)"},{"path":"small-worlds-and-large-worlds.html","id":"a-data-story","chapter":"2 Small Worlds and Large Worlds","heading":"2.3.1 A data story","text":"always great start lot times overlooked forgotten, .e. data generated. sometimes looking data way describe underlying property pattern looking gain knowledge underlying casual mechanisms drive underlying properites. Sometimes back forth can difficult (descriptive <-> causal) end model just trying give algorithm ’s built perform (remember golems). Thus, need understand story data generated important understanding model. addition, simulating data come handy helps get heart data generating process associated descriptive <-> causal linkages. simple summary might look stream proportion watershed model Data story.true proportion streams covering watershed \\(p\\).single random sample watershed probability \\(p\\) producing stream (S) observation.\nprobability 1 − \\(p\\) producing land (L) observation.random sample independent others.– McElreath (2018)much easier though see action sample update model understanding data story. ’ll go next section.","code":""},{"path":"small-worlds-and-large-worlds.html","id":"bayesian-updating","chapter":"2 Small Worlds and Large Worlds","heading":"2.3.2 Bayesian updating","text":"Remember ‘fish bucket’ example earlier followed paths come ways plausibilities, well ’s going process now ’ll just use random samples watershed instead. time sample watershed, ’ll update posterior plausibility taking prior multiplying likelihood. ’ll call Bayesian Updating can see process happening Figure 2.4. first, need get data (samples) show updates event., ’ll randomly sample stream raster Figure 2.3 get 9 samples either S (stream) L (land).vector sample9 get Land, Land, Land, Land, Land, Land, Stream, Land, Land.can see ’s pretty rare get stream land! first panel Figure 2.4 important shows prior (dashed line) uniform meaning give equal probability proportion stream watershed. give thought prior working example ’ll just stick uniform now. later chapters ’ll play prior see affects posterior distribution.\nFigure 2.4: Bayesian updating based random samples watershed. Notice get’s Land, knows can’t possible Land. Dashed line prior solid line likelihood.\n2.4 can see taking 50% probability getting ‘S’ ‘L’ (even though know ’s true…) updating based sample Land Stream. every time Land observed, peak plausibility curve moves left graph. repeats step changes likelihood depending priors.","code":"\nset.seed(1234)\n\nsample9 <- terra::spatSample(nhd_sutton, size = 9, method = \"random\", replace = TRUE)\n\n## just adding a S and L\nsample9$observations <- ifelse(is.na(sample9$nhdStrOrdRast), 'Land', 'Stream')"},{"path":"small-worlds-and-large-worlds.html","id":"evaluate","chapter":"2 Small Worlds and Large Worlds","heading":"2.3.3 Evaluate","text":"Richard goes details small large world comparisons warns practitioner’s assumptions data model, e.g. assuming model correct model use data small world representation large world. also makes important point data drawn. example, proportion streams watershed doesn’t matter draw stream 2, 3, 8, 20, etc order doesn’t matter situations order can matter ’d want make sure model aware order! addition, make sure aware models purpose (prediction, inference, etc) check adequacy within domain.","code":""},{"path":"small-worlds-and-large-worlds.html","id":"components-of-the-model","chapter":"2 Small Worlds and Large Worlds","heading":"2.4 Components of the model","text":"Now fun part, open machine! ’ll learn machine works, part part. help us connect model (golem) statistical properties, e.g. probability theory (distributions).","code":""},{"path":"small-worlds-and-large-worlds.html","id":"variables","chapter":"2 Small Worlds and Large Worlds","heading":"2.4.1 Variables","text":"Richard explains variables symbols take different values things wish infer (proportions rates) addition things actually observe like data. stream proportion example three variables; \\(p\\), \\(S\\), \\(L\\).first variable \\(p\\) proportion streams watershed. variable observed instead inferred data. Richard explains usually called parameter. variables just observed variables samples, e.g. stream land.","code":""},{"path":"small-worlds-and-large-worlds.html","id":"definitions","chapter":"2 Small Worlds and Large Worlds","heading":"2.4.2 Definitions","text":"variables need define . help us achieve ways can count paths considering variables used (observed unobserved). think define context little abstract Richard says really means walking paths. ’ll define observe unobserved variables help us abstract.","code":""},{"path":"small-worlds-and-large-worlds.html","id":"observed-variables","chapter":"2 Small Worlds and Large Worlds","heading":"2.4.2.1 Observed variables","text":"simply trying figure best way represent data generating process \\(S\\) \\(L\\) within watershed finding likely \\(p\\) given data. Typically data follows common probability distribution function (continuous pdf) probability mass function (discrete pmf) can use count us.example, two possible events: streams (\\(S\\)) land (\\(L\\)) also took 9 samples (\\(N\\)) watershed. like know plausible get sample configuration given amount samples.Couple things first, need make sure samples identically distributed independent (iid). ensures don’t bring sampling bias might impact machine (golem). Lucky us already distribution ! ’s call binomial distribution.\\[\nPr(W,L|p)=\\frac{(W+L)!}{W!L!}p^{W}(1-p)^{L}\n\\]side-bar–might thinking another distribution sounds similar also deals binary outcomes called Bernoulli. Bernoulli distribution probability \\(p\\) observing binary event.\\[\nPr(k|p)=p^{k}(1-p)^{1-k} \\\\ Pr(W,L|p)=p^{W}(1-p)^{L}\n\\]\nsuper similar binomial Bernoulli special case binomial essentially binomial one event!’s nice equation ’s already function R!relative number ways get 1 stream, holding \\(p\\) 0.5 \\(N = W+L\\) 9. can now summarize model. can change prob argument dbinom() see changes results. Richard talks Maximum entropy binomial distribution represents counting binary events explain Chapter 10.","code":"\ndbinom(1, size = 9, prob = .5)## [1] 0.01757813"},{"path":"small-worlds-and-large-worlds.html","id":"unobserved-variables","chapter":"2 Small Worlds and Large Worlds","heading":"2.4.2.2 Unobserved variables","text":"Since \\(p\\) observed ’ll call parameter even though can’t observe still need define somehow. can think parameters questions want answer: difference means groups, strong two variables associated, much variation measurements, etc. parameters, ’ll need assign prior. Just like previous section gave first prior uniform distribution (equally likely) ’ll need build models. might asking, “?”.can think priors ways help model learn (engineering assumptions) imparting knowledge reflect phenomenon (scientific assumtions). Essentially, priors can subjective non-subjective follow similar workflows, e.g. analyze data using prior information. Priors permanent law. assumptions treated like , meaning challenged explored!McElreath (2018) goes nice distinction parameter data also provides great summary shouldn’t get wrapped within Bayesian analyis.Rethinking: Datum parameter? typical conceive data parameters completely different kinds entities. Data measured known; parameters unknown must estimated data. Usefully, Bayesian framework distinction datum parameter fundamental. Sometimes observe variable, sometimes . case, distribution function applies, even though didn’t observe variable. result, assumption can look like “likelihood” “prior,” depending upon context, without change model. Much later book (Chapter 15), ’ll see exploit deep identity certainty (data) uncertainty (parameters) incorporate measurement error missing\ndata modeling.– McElreath (2018)","code":""},{"path":"small-worlds-and-large-worlds.html","id":"a-model-is-born","chapter":"2 Small Worlds and Large Worlds","heading":"2.4.3 A model is born","text":"Now ’ve done work define everything, let’s put together! just notation describing ’ve already done. observed variable \\(W\\) described binomial distribution size \\(N\\) probability \\(p\\).\\[\nW \\sim \\text{Binomial}(N,p)\n\\]unobserved parameter \\(p\\) just saying uniform (flat) ’s entire range (zero one):\\[\np \\sim \\text{Uniform}(0,1)\n\\]\nbest prior since know area way land stream (Figure 2.3. Later ’ll start play priors instead leaving flat.","code":""},{"path":"small-worlds-and-large-worlds.html","id":"making-the-model-go","chapter":"2 Small Worlds and Large Worlds","heading":"2.5 Making the model go","text":"’ll start getting guts Bayes’ theorem derive posterior don’t get discouraged ! really just process now derivation notation added gel rules probability theory. posterior distribution ’re going derive contains relative plausibility different parameter values, conditional data. First ’ll look joint probability \\(S, L, p\\).saying probability \\(S, L, p\\) product probability \\(Pr(S,L|p)\\) likelihood times prior probability \\(p\\). like saying probability rain cold day equal probability rain, ’s cold, times probability ’s cold (McElreath (2018)).’s also true reverse logic, e.g. reverse \\(p\\) \\(rhs\\).Meaning probability rain cold day equal probability cold, ’s raining, times probability rain (McElreath (2018)).Now equations equal, can just set equal .Now little algebra,Voila! now wanted first place, e.g. \\(Pr(p|S,L)\\). Bayes’ theorem, saying probability \\(p\\) given data (\\(S,L\\)), equal likelihood (plausibility data conditional \\(p\\)) times prior plausibility, divided average probability data.\\[\n\\text{Posterior}=\\frac{\\text{Probability data} \\times \\text{Prior}}{\\text{Average probability data}}\n\\]McElreath (2018), talks denominator Bayes’ theorem particularly confusing can different names (evidence, average likelihood) literally average probability data. ’s meant standardize posterior making sure \\(rhs\\) sums one.\\[\nPr(W,L) = E\\bigl(Pr(W,L|p)\\big) = \\int{Pr(W,L|p)Pr(p)dp}\n\\]key take away equation integral helps standardize average probability values sum one. Remember forking path’s example ways produce \\(\\times\\) prior plausibility, divided sum products didn’t probability. thing goes estimate, continuous discrete, start likelihood \\(\\times\\) prior remember likelihood priors just plausibilities means doesn’t necessarily need zero 1 (absolutely can likely )!McElreath (2018) finishes section showing Figure 2.5 can keep likelihood change prior effect posterior varies.\nFigure 2.5: Graph showing posterior varying choice prior.\n","code":""},{"path":"small-worlds-and-large-worlds.html","id":"motors","chapter":"2 Small Worlds and Large Worlds","heading":"2.5.1 Motors","text":"section, McElreath (2018) dives machine(s) makes possible compute posterior distribution, namely:Grid approximationQuadratic approximationMarkov chain Monte Carlo (MCMC)reason need little help start adding complexity models mathematics becomes difficult possible! addition, models like linear regression certain formalities associated constrain choice prior might help math useful bayesian approach. methods help us approximate posterior distribution necessary tool tool-kit bayesian analysis.","code":""},{"path":"small-worlds-and-large-worlds.html","id":"grid-approximation","chapter":"2 Small Worlds and Large Worlds","heading":"2.5.2 Grid approximation","text":"Deemed one simplest sampling techniques grid approximation. goal take continuous grid (parameters) sample finite grid values. means particular value \\(p'\\) going multiply likelihood prior point. machine repeat starts get clearer picture posterior distribution.method comes cost though really inefficient start scaling problem . now, great stepping stone see machine action.McElreath (2018) gives recipe grid approximation using stream watershed example.Define grid. means decide many points use estimating \nposterior, make list parameter values grid.Compute value prior parameter value grid.Compute likelihood parameter value.Compute unstandardized posterior parameter value, multiplying \nprior likelihood.Finally, standardize posterior, dividing value sum values.– McElreath (2018)code steps :\nFigure 2.6: Posterior distributions using samples 5 20.\nsamples, precision ’ll get. Let’s look one 1,000 samples.\nFigure 2.7: Posterior distribution using 1,000 samples.\nprior just 1, dbinom right? Yes. play around prior show effects posterior.\nFigure 2.8: Showing varying effect prior posterior\n","code":"\nd <- tibble(p_grid = seq(from = 0, to = 1, length.out = 20),      # define grid\n           prior  = 1)  %>%                                       # define prior\n     mutate(likelihood = dbinom(1, size = 9, prob = p_grid)) %>%  # compute likelihood at each value in grid\n     mutate(unstd_posterior = likelihood * prior) %>%             # compute product of likelihood and prior\n     mutate(posterior = unstd_posterior / sum(unstd_posterior))   # standardize the posterior, so it sums to 1\nprior  = ifelse(p_grid > 0.25, 1, 0.05)\n# or\nprior  = exp( -5*abs( p_grid - 0.5 ))"},{"path":"small-worlds-and-large-worlds.html","id":"quadratic-approximation","chapter":"2 Small Worlds and Large Worlds","heading":"2.5.3 Quadratic approximation","text":"Due downfalls grid approximation (efficiency, scalability) methods can help issues relatively easy. One useful approach quadratic approximation. look 2.7, can see little imagination posterior resembles Gaussian (normal) shape. ’ll take advantage shape posterior apply Gaussian approximation aka quadratic. logarithm Gaussain distribution forms parabola parabola quadratic function. can work really well linear regression efficient ’ll use first following sections get complex.McElreath (2018) lays couple steps working quadratic approximation ’ll dive .Find posterior mode. usually accomplished optimization algorithm,\nprocedure virtually “climbs” posterior distribution, \nmountain. golem doesn’t know peak , know slope\nfeet. many well-developed optimization procedures, \nclever simple hill climbing. try find peaks.find peak posterior, must estimate curvature near \npeak. curvature sufficient compute quadratic approximation \nentire posterior distribution. cases, calculations can done analytically,\nusually computer uses numerical technique instead.– McElreath (2018)","code":""},{"path":"sampling-the-imaginary.html","id":"sampling-the-imaginary","chapter":"3 Sampling the Imaginary","heading":"3 Sampling the Imaginary","text":"describe methods chapter.Math can added body using usual syntax like ","code":""},{"path":"sampling-the-imaginary.html","id":"math-example","chapter":"3 Sampling the Imaginary","heading":"3.1 math example","text":"\\(p\\) unknown expected around 1/3. Standard error approximated\\[\nSE = \\sqrt(\\frac{p(1-p)}{n}) \\approx \\sqrt{\\frac{1/3 (1 - 1/3)} {300}} = 0.027\n\\]can also use math footnotes like this1.approximate standard error 0.0272","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
